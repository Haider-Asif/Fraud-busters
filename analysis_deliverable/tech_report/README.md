# Tech Report

#### A defined hypothesis or prediction task, with clearly stated metrics for success. ####
The prediction task is that we need to determine whether a transaction, given by its 30 attributes in the database, is fraudulent or not. The metrics for success is how well our model is able to predict whether a transaction is a fraud or not correctly.  
One special case of defining success in this case would be getting the model to classify only fraudulent transactions as fraud correctly. The reason for this is that the data set is extremely skewed, with less than one percent of the transactions being fraud, which means that it is really easy for the model to achieve very high overall accuracy while not doing well on the fraud transaction subset. This is why our goal was to optimize the model in order to get the maximum accuracy on the fraudulent transactions, or in lay terms, correctly identifying when a transaction is fraud rather than just correctly identifying that it’s not.   
 
#### Why did you use this statistical test or ML algorithm? Which other tests did you consider or evaluate? How did you measure success or failure? Why that metric/value? What challenges did you face evaluating the model? Did you have to clean or restructure your data? ####
The ML algorithm we used was L2 Regularized Logistic regression. The reason we used this algorithm was because we wanted to make a binary classification while using supervised learning so that the model can learn which transactions would be fraudulent or not. We also wanted to use stochastic gradient descent in it to optimize the loss of the model in order to get better results but we ended up using sklearn’s built in Logistic regression function which would do this for us. We also had the default L2 regularization for the model so that incorrect classifications are penalized higher and hence better results are achieved faster while at the same time results do not overfit to the training dataset and the model remains generalizable.   
We considered decision trees and kmeans clusters as alternative models to this classification problem. We are still considering using kmeans in our final submission to juxtapose the predictions of a both a supervised and an unsupervised learning model. We used binary logistic regression because our classification problem is binary, either fraud or not, and we thought it would best be able to fit and analyze the data. Since the data is mostly non-fraud transactions and since the crux of the problem is identifying the fraud ones, we measure the accuracy of our model by figuring the percentage of correct predictions for fraud and non-fraud transactions independently. Among the challenges we faced was the overwhelming number of non-fraud cases as opposed to fraud cases; thus, the model had a tendency to evaluate everything as non-fraud to score higher accuracies. We didn’t need to clean the data at this step, since we have already cleaned it at the previous checkpoint. We imported the data from the database and used all the fields there except for the id, and we experimented with trying to eliminate the time field from the training parameters, and we scored higher accuracies there at a lower number of iterations, but then it plateaued. The accuracies whether the time is considered or not are very comparable, even though it is a notch higher without the time field.   
 
#### What is your interpretation of the results? Do accept or deny the hypothesis, or are you  satisfied with your prediction accuracy? For prediction projects, we expect you to argue why you got the accuracy/success metric you have. Intuitively, how do you react to the results? Are you confident in the results? ####
Our final model, which did include all the 30 features including the relative times, has a top accuracy of 0.635 for only fraudulent transactions, 0.9998 for non-fraudulent transactions and 0.996 for all transactions. We saw that the accuracy for the non-fraudulent transactions and the whole dataset, because of the skew of the dataset did not change much with the increase of the total number of transactions; 1-100 iterations would yield a change of almost 0.003 in accuracy which is next to nothing. However, for the fraudulent transactions, the accuracy kept increasing generally apart from some occasional dips, and was the maximum after 165 iterations, we saw that after this the model started overfitting and the accuracy started gradually decreasing, which is why we kept this as our optimal number of iterations. We believe that intuitively, we got a lower number for the accuracy in fraudulent subset, because of the lack of training data with these labels, because of this the model is not able to learn what constitutes a fraudulent transaction as well as we would want it to, and hence leads to a smaller accuracy. However, considering how small the number of fraudulent transactions there are in the dataset, we are particularly proud of reaching even this accuracy. We are confident that these results are good, and the model generalizable, and that with more data, we could get even better accuracies.   
 
#### For your visualization, why did you pick this graph? What alternative ways might you communicate the result? Were there any challenges visualizing the results, if so, what were they? Will your visualization require text to provide context or is it standalone (either is fine, but it recognizes which type your visualization is)? ####
We thought a double line graph best represents both the relationship between accuracies of the same class with different number of iterations and the relationship between both accuracies (that of fraud and non-fraud transactions).  Firstly, the two lines on the same coordinate plane show the difference in the model’s performance based on the class of the datapoint. It allows the analyst to spot the difference between them instantly and determine where the model needs fixing. Secondly, it compares the accuracies across the number of iterations, which allows the analyst to easily spot the maximum of the graph and see which number of iterations show the best results. It also shows the relationship between the number of iterations and the accuracy so that the analyst is able to spot where the model converges, where it starts to overfit, where it may have underfitted the data, and so on. 
 
#### Full results + graphs (at least 1 stats/ml test and at least 1 visualization). You should push your visualizations to the /analysis_deliverable/visualizations folder in your repo. Depending on your model/test/project we would ideally like you to show us your full process so we can evaluate how you conducted the test! ####
Please check the /analysis_deliverable/visualizations path to view our plots. We have two plats there, one that represents that accuracies without the data field and the other represents it with the data field. The plots are saved as images are named with_time_accuracy_plot.png (that reflects the result taking the time field in consideration) and without_time_acc_plot.png (that ignores the time field).
 
#### If you did a machine learning model, why did you choose this machine learning technique? Does your data have any sensitive/protected attributes that could affect your machine learning model? ####
We used L2 regularized logistic regression and gave our reasoning for it earlier in the report. Our data is very obscure, meaning that the data was curated so that all fields are unknown and just have a numerical representation. This was done by the creators of the dataset so that it protects the identity of the users of the credit cards. If we know what these fields are, we would have been able to make better assumptions about which fields to include or exclude, and which fields may introduce bias that we may need to balance. However, due to the sensitivity of the information and limitation of our dataset, we are not able to take these factors in consideration while learning the patterns in the dataset. 


We ***highly encourage you to use markdown syntax to organize and clean your reports*** since it will make it a lot more comprehenisble for the TA grading your project.
